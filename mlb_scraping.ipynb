{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 1.283 seconds url: https://www.baseball-reference.com/boxes/CHN/CHN201810020.shtml\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "import traceback\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup , Comment \n",
    "from lxml import etree\n",
    "\n",
    "#test\n",
    "def newtag(bs : BeautifulSoup, tagn : str ,text: str) :\n",
    "    tag = bs.new_tag(tagn)\n",
    "    tag.string = text \n",
    "    return tag\n",
    "\n",
    "def load_mlb_schedule(year):\n",
    "    \n",
    "    BASE_URL = \"https://www.baseball-reference.com\" \n",
    "    link = f\"{BASE_URL}/leagues/MLB/{year}-schedule.shtml\"\n",
    "    schedule = []\n",
    "    gdir = str(year)\n",
    "    dummycnt = 1\n",
    "    \n",
    "    try:\n",
    "        #crear el repots\n",
    "        if not (path.isdir(gdir)):\n",
    "            os.mkdir(gdir)\n",
    "\n",
    "        #carga el archivo de los datos\n",
    "        #file = open(\"mlb_game.txt\",\"a\")\n",
    "\n",
    "        # cargar la pagina\n",
    "        time.sleep(3)\n",
    "        tic = time.perf_counter()\n",
    "        reps = requests.get(link)\n",
    "        toc = time.perf_counter()\n",
    "        \n",
    "        #leer el contenido de la pagina\n",
    "        html = reps.content\n",
    "        if (html):  \n",
    "            print(f\"Schedule {toc - tic+1:0.4} seconds url: {link}\")\n",
    "            soup = BeautifulSoup(html,'html.parser')\n",
    "            div_section_wrapper = soup.find_all('div',class_= 'section_wrapper')\n",
    "            div_section_content = div_section_wrapper[0].find_all('div',class_='section_content')\n",
    "            divs = div_section_content[0].find_all('div')\n",
    "            print(len(divs))\n",
    "            for div in divs:\n",
    "                ps = div.find_all('p', class_=\"game\")\n",
    "                for p in ps:\n",
    "                    ems = p.find_all('em')\n",
    "                    for em in ems:\n",
    "                        if dummycnt == 1 :\n",
    "                            schedule.append(f\"{BASE_URL}{em.a.get('href')}\") \n",
    "                        if dummycnt % 2 == 0 :\n",
    "                            schedule.append(f\"{BASE_URL}{em.a.get('href')}\")\n",
    "                        dummycnt += 1   \n",
    "        return schedule\n",
    "    except ValueError:\n",
    "            print(\"Error de Valor\")\n",
    "            traceback.print_exception(*sys.exc_info())\n",
    "            return schedule\n",
    "    except:    \n",
    "            print(\"Error de general\") \n",
    "            traceback.print_exception(*sys.exc_info())   \n",
    "            return schedule                     \n",
    "\n",
    "def load_mlb_games(link,folder):\n",
    "    \n",
    "\n",
    "    try:\n",
    "\n",
    "        #carga el archivo de los datos\n",
    "        file = open(str(folder)+'//'+link.split('/')[5],'a')\n",
    "        gamedate = link.split('/')[5][3:12]                                    \n",
    "\n",
    "        # cargar la pagina\n",
    "        time.sleep(3)\n",
    "        tic = time.perf_counter()\n",
    "        reps = requests.get(link)\n",
    "        toc = time.perf_counter()\n",
    "        \n",
    "        #leer el contenido de la pagina\n",
    "        html = reps.content\n",
    "        if (html):  \n",
    "            print(f\"Process {toc - tic+1:0.4} seconds url: {link}\")\n",
    "            soup = BeautifulSoup(html,'html.parser')\n",
    "           \n",
    "            #codigo de los equipos\n",
    "            uls = soup.find_all('ul',class_='in_list')\n",
    "            teams = uls[0].find_all('a')\n",
    "            home  = teams[1]['href'].split('/')[2]\n",
    "            visi  = teams[2]['href'].split('/')[2]\n",
    "\n",
    "            #saca el tag escondido\n",
    "            comments = soup.find_all(text=lambda text:isinstance(text, Comment))\n",
    "            htm = str([htm for htm in comments if \"div_play_by_play\" in htm]);\n",
    "            if(htm):                          \n",
    "                div = BeautifulSoup(htm,'html.parser')\n",
    "                tbody  = div.find_all(\"tbody\")\n",
    "                rows = tbody[0].find_all('tr', class_ =['top_inning','bottom_inning'])\n",
    "               \n",
    "                for row in rows:\n",
    "                    if row != ['']:\n",
    "                       thtext = row.find_all('th')[0].text                     \n",
    "                       row.insert(0, newtag(div,'td',thtext))\n",
    "                       row.insert(0, newtag(div,'td',visi))\n",
    "                       row.insert(0, newtag(div,'td',home))\n",
    "                       row.insert(0, newtag(div,'td',gamedate))                     \n",
    "\n",
    "                      # print(row)\n",
    "                       cols=row.find_all('td')\n",
    "                       cols=[\" \".join(x.text.replace(',','|').upper().split()) for x in cols]\n",
    "                       file.write(\" , \".join(cols)+'\\n')\n",
    "                       #print(cols)  \n",
    "            file.close  \n",
    "    except ValueError:\n",
    "            file.close \n",
    "            print(\"Error de Valor\")\n",
    "            traceback.print_exception(*sys.exc_info())\n",
    "    except:    \n",
    "            file.close                     \n",
    "            print(\"Error de general\") \n",
    "            traceback.print_exception(*sys.exc_info())   \n",
    "\n",
    "#load_mlb_schedule(2018) \n",
    "score  = 'https://www.baseball-reference.com/boxes/CHN/CHN201810020.shtml'                                            \n",
    "load_mlb_games(score,2018)\n",
    "                                \n",
    "#for score in load_mlb_schedule(2018):\n",
    "    #load_mlb_games(score,2018)\n",
    "                                \n",
    "                               \n",
    "                                \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHN201810020.shtml'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
