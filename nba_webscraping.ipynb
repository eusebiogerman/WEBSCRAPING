{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nba_webscraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/eusebiogerman/WEBSCRAPING/blob/master/nba_webscraping.ipynb",
      "authorship_tag": "ABX9TyNTP2Rw8rs+CUbHqRrVfnaR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eusebiogerman/WEBSCRAPING/blob/master/nba_webscraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_eBNey3L3F7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "249c7d1a-0a97-4fd7-93ef-2d68502a45ea"
      },
      "source": [
        "import unittest\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "from os import path\n",
        "import traceback\n",
        "import requests\n",
        "import logging\n",
        "\n",
        "from console_progressbar import ProgressBar\n",
        "from bs4 import BeautifulSoup , Comment \n",
        "from lxml import etree\n",
        "\n",
        "#test\n",
        "def newtag(bs : BeautifulSoup, tagn : str ,text: str) :\n",
        "    tag = bs.new_tag(tagn)\n",
        "    tag.string = text \n",
        "    return tag\n",
        "\n",
        "def get_trace_value(plist):\n",
        "    trace = ''\n",
        "    for p in plist:\n",
        "        trace = trace + str(p) + '/n'\n",
        "    return trace\n",
        "\n",
        "def load_nba_schedule(year):\n",
        "    \n",
        "    #BASE_URL = \"https://www.baseball-reference.com\" \n",
        "    BASE_URL = \"https://www.basketball-reference.com/\"\n",
        "    link = f\"{BASE_URL}/leagues/NBA_{year}_games.html\"\n",
        "    link2 = ''\n",
        "    archivo = ''\n",
        "    fecha = ''\n",
        "    schedule = []\n",
        "    gdir = str(year)\n",
        "    dummycnt = 1\n",
        "    \n",
        "    try:\n",
        "        #crear el repots\n",
        "        if not (path.isdir('sessons')):\n",
        "            os.mkdir('sessons')\n",
        "\n",
        "        #carga el archivo de los datos\n",
        "        #file = open(\"mlb_game.txt\",\"a\")\n",
        "\n",
        "        # cargar la pagina\n",
        "        time.sleep(3)\n",
        "        tic = time.perf_counter()\n",
        "        reps = requests.get(link)\n",
        "        toc = time.perf_counter()\n",
        "        \n",
        "        #leer el contenido de la pagina\n",
        "        html = reps.content\n",
        "        if (html):  \n",
        "            #Obtengo el cuerpo de los datos(div,table,tbody) \n",
        "            #print(f\"Schedule {toc - tic+1:0.4} seconds url: {link}\")\n",
        "            soup = BeautifulSoup(html,'html.parser')\n",
        "            div_schedule = soup.find_all('div', {'id','div_schedule'})\n",
        "            table = soup.find_all('table')\n",
        "            tb = table[0].find_all('tbody')\n",
        "            trs = tb[0].find_all('tr')\n",
        "            #lectura y almacenamientos de los datos\n",
        "            for tr in trs:\n",
        "                tds = tr.find_all('td', {'class','center'})\n",
        "                for td in tds:\n",
        "                    a = td.find_all('a')\n",
        "                    if (a) :\n",
        "                       dict = {}\n",
        "                       fecha = a[0].get('href').split('/')[2][0:9]\n",
        "                       archivo = f\"sessons/{gdir}.txt\"\n",
        "                       link2 =  f\"{BASE_URL}{a[0].get('href')}\"\n",
        "                       dict ={'link' : link2, 'archivo' : archivo, 'fecha' : fecha }  \n",
        "                       schedule.append(dict)\n",
        "        return schedule\n",
        "    except ValueError:\n",
        "            print(\"Error de Valor\")\n",
        "            traceback.print_exception(*sys.exc_info())\n",
        "            return schedule\n",
        "    except:    \n",
        "            print(\"Error de general\") \n",
        "            traceback.print_exception(*sys.exc_info())   \n",
        "            return schedule                     \n",
        "\n",
        "\n",
        "def load_nba_games(link, archivo, gamedate):\n",
        "    \n",
        "    logfile = archivo.split('/')[1].replace('txt','log')\n",
        "    logfile = archivo.split('/')[0] +'/'+ logfile\n",
        "    logging.basicConfig(filename = 'test.log', enconding ='utf-8',level=logging.INFO)\n",
        "    plays = [] \n",
        "\n",
        "    try:\n",
        "        #carga el archivo de los datos\n",
        "        file = open(archivo, 'a')\n",
        "        # cargar la pagina\n",
        "        time.sleep(3)\n",
        "        tic = time.perf_counter()\n",
        "        reps = requests.get(link)\n",
        "        toc = time.perf_counter()\n",
        "        #leer el contenido de la pagina\n",
        "        html = reps.content\n",
        "        if (html):  \n",
        "            logging.info(f\"Process {toc - tic+1:0.4} seconds url: {link}\")\n",
        "            soup = BeautifulSoup(html,'html.parser')\n",
        "            #tag escondido para buscar los codigos de los equipos\n",
        "            comments = soup.find_all(text=lambda text:isinstance(text, Comment))\n",
        "            htm = str([htm for htm in comments if \"div_other_scores\" in htm]);\n",
        "            htmwrap = BeautifulSoup(htm,'html.parser')\n",
        "            te_as =  htmwrap.find_all('div',class_ ='game_summary nohover current')[0].find_all('a')\n",
        "            teams = [teams.text for teams in te_as]\n",
        "            del teams[1]\n",
        "            #codigo de los equipos\n",
        "            div_qs = soup.find_all('div',class_='filter switcher')\n",
        "            divbox = div_qs[0].find_all('div')\n",
        "            divbox = [x.a.get('data-show').split(',') for x in divbox]\n",
        "            del divbox[0]\n",
        "            #obtengo los contenedores de las jugadas    \n",
        "            for box_out in divbox:\n",
        "                for box in box_out:\n",
        "                    qter = box[-1]\n",
        "                    divb = f\"{box[1:]}-basic\"\n",
        "                    #Obtengo el cuerpo de los datos(div,table,tbody) \n",
        "                    dbx = soup.find_all('div',{'id':f\"div_{divb}\"})\n",
        "                    tb   = dbx[1].find_all('table',{'id':divb})\n",
        "                    tbo  = tb[0].find_all('tbody')\n",
        "                    trs  = tbo[0].find_all('tr')\n",
        "                    #lectura y almacenamientos de los datos\n",
        "                    for tr in trs :\n",
        "                        player = tr.find_all('th')[0].text.upper()\n",
        "                        tds  = tr.find_all('td')\n",
        "                        if (len(tds) > 1):\n",
        "                            tds  = [\" \".join(x.text.replace(',','|').upper().split()) for x in tds]\n",
        "                            tds.insert(0,int(tds[0].replace(':','')))\n",
        "                            tds.insert(0,qter)\n",
        "                            tds.insert(0,player)\n",
        "                            tds.insert(0,teams[0])\n",
        "                            tds.insert(0,teams[1])\n",
        "                            tds.insert(0,gamedate)\n",
        "                            plays.append(tds)\n",
        "\n",
        "            plays = sorted(plays, key = lambda x :(x[4],x[5]))  \n",
        "            for y in plays:\n",
        "                del y[5] \n",
        "                file.write(\",\".join(y)+'\\n')\n",
        "            #return plays\n",
        "    except ValueError:\n",
        "            #file.close \n",
        "            print(\"Error de Valor : load_mlb_games \")\n",
        "            logging.error(get_trace_value(sys.exc_info()))\n",
        "    except:    \n",
        "            #file.close                 \n",
        "            print(\"Error de general : load_mlb_games\") \n",
        "            logging.error(get_trace_value(sys.exc_info()))\n",
        "'''\n",
        "sche = load_nba_schedule(2018) \n",
        "for es in sche:\n",
        "    print(es)\n",
        "'''\n",
        "year = 2019\n",
        "sesson  = load_nba_schedule(year)\n",
        "cnt = len(sesson)\n",
        "pr = 1\n",
        "pb = ProgressBar(total=cnt,prefix='Downloading Games ' + str(year) ,suffix=' ',decimals=2,length=50,fill='‚ñê',zfill=' ')\n",
        "for score in sesson:\n",
        "    load_nba_games(score['link'],score['archivo'],score['fecha'])\n",
        "    pb.print_progress_bar(pr)\n",
        "    pr = pr + 1\n",
        "\n",
        "#score  = {'link': 'https://www.basketball-reference.com//boxscores/201710230DEN.html', 'archivo': 'sessons/2018.txt', 'fecha': '201710230'}\n",
        "#load_nba_games(score['link'],score['archivo'],score['fecha'])\n",
        "\n",
        "\n",
        "# https://www.basketball-reference.com/leagues/NBA_2018_games.html\n",
        "# https://www.basketball-reference.com/teams/DEN/2021_games.html\n",
        "# https://www.basketball-reference.com/boxscores/202012230POR.html\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Games 2019 |‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê           | 79.09%  Error de general : load_mlb_games\n",
            "Downloading Games 2019 |‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê          | 80.00%  Error de general : load_mlb_games\n",
            "Downloading Games 2019 |‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê          | 81.82%  Error de general : load_mlb_games\n",
            "Downloading Games 2019 |‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê         | 82.73%  Error de general : load_mlb_games\n",
            "Downloading Games 2019 |‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê‚ñê| 100.00%  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk7YZafFCPIg"
      },
      "source": [
        "# Secci√≥n nueva"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB9TY5UDCtEB",
        "outputId": "b2aa82f7-0199-4b50-998b-c5df6acd73a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ls /content/drive/myDrive/WEBSCRPING/NBA/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/drive/myDrive/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8PS90kHCyky",
        "outputId": "d25dd895-3bfc-4246-a8d1-e28ead578e97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvcxrMYUDrEz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}